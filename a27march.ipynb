{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67db37bf-dbee-49e2-8a12-68b30e9e32ed",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bb5fce-1bea-4ad1-aecb-728f832a0073",
   "metadata": {},
   "source": [
    "R-squared, also known as the coefficient of determination, is a statistical measure that represents the proportion of the variance in the dependent variable that can be explained by the independent variables in a linear regression model. It provides an indication of how well the model fits the data.\n",
    "\n",
    "R-squared is calculated by dividing the explained sum of squares (ESS) by the total sum of squares (TSS). ESS represents the sum of the squared differences between the predicted values and the mean of the dependent variable. TSS represents the sum of the squared differences between the actual values and the mean of the dependent variable. The formula for R-squared is:\n",
    "\n",
    "R-squared = ESS / TSS\n",
    "\n",
    "R-squared ranges from 0 to 1, where 0 indicates that the independent variables do not explain any of the variability in the dependent variable, and 1 indicates that the independent variables explain all of the variability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0c17a5-c06a-465c-9c57-1117ae3a8356",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b165a2-6e9c-4722-8f70-ba34563db47d",
   "metadata": {},
   "source": [
    "Adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables in the model and adjusts the value accordingly. It addresses the issue of overfitting by penalizing the addition of unnecessary variables to the model. Unlike R-squared, which always increases with the addition of more variables, adjusted R-squared can decrease if the added variables do not significantly contribute to the model's predictive power.\n",
    "\n",
    "The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - (1 - R-squared) * (n - 1) / (n - k - 1)\n",
    "\n",
    "where n is the number of observations and k is the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb5523b-54ae-4ff1-b8de-979c54db2bf9",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5db9c1-4bde-4489-bc45-a8cad2052160",
   "metadata": {},
   "source": [
    "Adjusted R-squared is more appropriate to use when comparing models with different numbers of independent variables. It provides a more reliable measure of the model's goodness of fit by taking into account the complexity of the model and the potential for overfitting. Adjusted R-squared penalizes the addition of unnecessary variables, discouraging the inclusion of variables that do not improve the model's performance. Therefore, when comparing models with different numbers of independent variables, adjusted R-squared can help identify the model that strikes a balance between goodness of fit and model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d76ea0-ec01-4cc1-92a7-9a7ffff5d2f9",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37f95c-8b7e-4450-8c1c-c8d61213f1df",
   "metadata": {},
   "source": [
    "In regression analysis, RMSE stands for Root Mean Squared Error, MSE stands for Mean Squared Error, and MAE stands for Mean Absolute Error.\n",
    "\n",
    "RMSE is calculated by taking the square root of the average of the squared differences between the predicted values and the actual values. It represents the standard deviation of the residuals and provides a measure of the average prediction error.\n",
    "\n",
    "MSE is calculated by taking the average of the squared differences between the predicted values and the actual values. It is a measure of the average squared prediction error.\n",
    "\n",
    "MAE is calculated by taking the average of the absolute differences between the predicted values and the actual values. It provides a measure of the average absolute prediction error.\n",
    "\n",
    "These metrics are used to assess the performance of a regression model and quantify the accuracy of its predictions. Lower values indicate better performance, as they indicate smaller prediction errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1248d2a-7b0d-498e-9cbb-bcb6b664d887",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc82ca4-2fdf-4322-9064-d8c27a745488",
   "metadata": {},
   "source": [
    "Advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "RMSE, MSE, and MAE provide a quantitative measure of prediction error, allowing for easy comparison between models.\n",
    "They consider both positive and negative errors, providing a balanced view of the model's performance.\n",
    "These metrics are widely used and well-understood in the field of regression analysis.\n",
    "RMSE and MSE give more weight to larger errors, which may be important in certain applications.\n",
    "Disadvantages:\n",
    "\n",
    "These metrics are sensitive to outliers since squared differences in RMSE and MSE magnify the effect of outliers.\n",
    "They do not provide direct interpretability, making it challenging to understand the practical implications of the errors.\n",
    "The choice of these metrics does not take into account specific requirements or costs associated with different types of errors. For example, in some cases, overestimating a certain outcome might have different consequences than underestimating it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d202e995-0546-4676-93b6-7b1aa80ab752",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b113b3-e848-46a3-8b62-865cb06e7e6f",
   "metadata": {},
   "source": [
    "Lasso regularization is a technique used in linear regression to add a penalty term to the loss function, encouraging sparse solutions by forcing some regression coefficients to become exactly zero. It differs from Ridge regularization in that Lasso can perform both variable selection and parameter shrinkage.\n",
    "\n",
    "While Ridge regularization (L2 regularization) shrinks the coefficients towards zero, Lasso regularization (L1 regularization) can shrink the coefficients to zero. This property of Lasso allows it to perform automatic feature selection by effectively excluding irrelevant features from the model.\n",
    "\n",
    "Lasso regularization is more appropriate when there is a belief or evidence that only a subset of the independent variables is relevant or when the goal is to obtain a simpler model with a reduced number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f87db16-e040-445e-9064-8a768ba6b073",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726f569e-3804-47b5-a2b2-66d6f0203e06",
   "metadata": {},
   "source": [
    "Regularized linear models help prevent overfitting in machine learning by adding a penalty term to the loss function, which discourages complex models with high coefficients. By limiting the magnitude of the coefficients, regularization prevents the model from becoming too sensitive to the training data, thereby improving its generalization performance on unseen data.\n",
    "\n",
    "For example, consider a regularized linear regression model trained to predict housing prices based on features such as square footage, number of bedrooms, and location. Without regularization, the model might fit the training data extremely well but perform poorly on new data due to overfitting. By applying regularization, the model will shrink the coefficients of less important features, reducing the chances of overfitting and improving its ability to generalize to new houses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c248b2-c7b4-4ea8-986c-428b9f552dd5",
   "metadata": {},
   "source": [
    "# Quetion : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc130129-b3d6-4005-84f1-c17b7a241217",
   "metadata": {},
   "source": [
    " Limitations of regularized linear models in regression analysis include:\n",
    "\n",
    "Feature dependency: Regularization assumes that the relationship between the dependent variable and independent variables is linear. If the relationship is highly nonlinear or exhibits complex interactions, regularized linear models may not capture it effectively.\n",
    "\n",
    "Loss of information: Regularization can shrink the coefficients towards zero, leading to some degree of information loss. If all the independent variables are truly relevant, excessive regularization may lead to underfitting and suboptimal model performance.\n",
    "\n",
    "Parameter selection: Regularized linear models require tuning of regularization parameters, such as the strength of the penalty term. Selecting the optimal parameter values can be challenging, and different values may lead to varying model performance.\n",
    "\n",
    "Interpretability: The penalty term added by regularization can make the interpretation of the model coefficients more complex. It becomes difficult to directly attribute the impact of each independent variable on the dependent variable, particularly when coefficients are significantly shrunken."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bff2d3-0ac9-40ad-9f5c-7964f731001e",
   "metadata": {},
   "source": [
    "# Quetion : 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f38537e-7394-45e3-a773-df68bfa04534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b7a088a7-c47a-4512-a3b7-06e192778a83",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
